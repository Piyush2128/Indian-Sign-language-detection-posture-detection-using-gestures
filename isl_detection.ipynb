{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c5d758-cf19-4f65-99bf-45569f11c883",
   "metadata": {},
   "source": [
    "# Indian Sign Language Detection using Gestures\n",
    "\n",
    "## 1. Problem Definition & Objective\n",
    "\n",
    "### Problem Statement\n",
    "Indian Sign Language (ISL) is a primary means of communication for the\n",
    "deaf and hard-of-hearing community. However, the lack of automated,\n",
    "real-time interpretation systems limits accessibility.\n",
    "\n",
    "### Objective\n",
    "To build a real-time Indian Sign Language detection system using\n",
    "hand gestures captured via a webcam and classified using a deep\n",
    "learning model.\n",
    "\n",
    "### Real-World Application\n",
    "Assistive communication, inclusive education, and human–computer interaction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb13b2ef-e85f-4f7e-b7f6-d1f4923e880f",
   "metadata": {},
   "source": [
    "## 2. Data Understanding & Preparation\n",
    "\n",
    "### Data Source\n",
    "Hand gestures are captured in real time using a webcam.\n",
    "\n",
    "### Feature Extraction\n",
    "- 21 hand landmarks detected using MediaPipe\n",
    "- (x, y) coordinates extracted per landmark\n",
    "- Relative normalization applied\n",
    "\n",
    "### Preprocessing\n",
    "- Landmark normalization\n",
    "- Conversion to a flattened feature vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102090b6-30e0-4f1a-a357-6c48d3755bc3",
   "metadata": {},
   "source": [
    "## 3. Model / System Design\n",
    "\n",
    "### AI Technique Used\n",
    "Deep Learning (Neural Network)\n",
    "\n",
    "### System Architecture\n",
    "Webcam → MediaPipe Hand Detection → Landmark Processing →\n",
    "Trained Neural Network → Gesture Prediction\n",
    "\n",
    "### Justification\n",
    "MediaPipe enables fast and accurate hand tracking suitable\n",
    "for real-time applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58223763-560d-4406-8460-12143fe8beaf",
   "metadata": {},
   "source": [
    "## 4. Core Implementation\n",
    "\n",
    "### Libraries Used\n",
    "- OpenCV\n",
    "- MediaPipe\n",
    "- TensorFlow / Keras\n",
    "- NumPy, Pandas\n",
    "\n",
    "### Implementation Overview\n",
    "The trained model (`model.h5`) is loaded and used to predict\n",
    "hand gestures in real time based on extracted landmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ac3c9-b8c1-4bf5-89ec-fd86ea0aedbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "MODEL_PATH = os.path.join(BASE_DIR, \"model.h5\")\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"model.h5 not found in {BASE_DIR}\")\n",
    "\n",
    "model = keras.models.load_model(MODEL_PATH)\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "alphabet = ['1','2','3','4','5','6','7','8','9']\n",
    "alphabet += list(string.ascii_uppercase)\n",
    "\n",
    "\n",
    "def calc_landmark_list(image, landmarks):\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "    landmark_point = []\n",
    "\n",
    "    for landmark in landmarks.landmark:\n",
    "        landmark_x = min(int(landmark.x * image_width), image_width - 1)\n",
    "        landmark_y = min(int(landmark.y * image_height), image_height - 1)\n",
    "        landmark_point.append([landmark_x, landmark_y])\n",
    "\n",
    "    return landmark_point\n",
    "\n",
    "\n",
    "def pre_process_landmark(landmark_list):\n",
    "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
    "\n",
    "    base_x, base_y = temp_landmark_list[0][0], temp_landmark_list[0][1]\n",
    "    for i in range(len(temp_landmark_list)):\n",
    "        temp_landmark_list[i][0] -= base_x\n",
    "        temp_landmark_list[i][1] -= base_y\n",
    "\n",
    "    temp_landmark_list = list(itertools.chain.from_iterable(temp_landmark_list))\n",
    "\n",
    "    max_value = max(list(map(abs, temp_landmark_list)))\n",
    "    if max_value == 0:\n",
    "        return temp_landmark_list\n",
    "\n",
    "    temp_landmark_list = [x / max_value for x in temp_landmark_list]\n",
    "    return temp_landmark_list\n",
    "\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ") as hands:\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        image = cv2.flip(image, 1)\n",
    "        image.flags.writeable = False\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image_rgb)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "\n",
    "                landmark_list = calc_landmark_list(image, hand_landmarks)\n",
    "                pre_processed_landmark_list = pre_process_landmark(landmark_list)\n",
    "\n",
    "                # Draw landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                    mp_drawing_styles.get_default_hand_connections_style()\n",
    "                )\n",
    "\n",
    "               \n",
    "                df = pd.DataFrame([pre_processed_landmark_list])\n",
    "\n",
    "               \n",
    "                predictions = model.predict(df, verbose=0)\n",
    "                predicted_class = np.argmax(predictions)\n",
    "                confidence = np.max(predictions)\n",
    "\n",
    "                label = alphabet[predicted_class]\n",
    "\n",
    "                \n",
    "                cv2.putText(\n",
    "                    image,\n",
    "                    f\"{label} ({confidence:.2f})\",\n",
    "                    (30, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1.5,\n",
    "                    (0, 0, 255),\n",
    "                    3\n",
    "                )\n",
    "\n",
    "        cv2.imshow(\"Indian Sign Language Detector\", image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:  # ESC key\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87510e52-c2cc-490f-a402-f590c2b2e04f",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Analysis\n",
    "\n",
    "### Evaluation Method\n",
    "Qualitative evaluation using real-time webcam predictions.\n",
    "\n",
    "### Observations\n",
    "- Correct detection for clear hand postures\n",
    "- Real-time inference with low latency\n",
    "\n",
    "### Limitations\n",
    "- Sensitive to lighting\n",
    "- Limited gesture classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79b5826-1623-41f4-8c67-83bcfb22f59e",
   "metadata": {},
   "source": [
    "## 6. Ethical Considerations & Responsible AI\n",
    "\n",
    "### Bias\n",
    "The system may not generalize equally across all users due\n",
    "to limited gesture variations.\n",
    "\n",
    "### Responsible Use\n",
    "Designed strictly for assistive and educational purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5f9bd4-2336-4859-8f67-289d57063528",
   "metadata": {},
   "source": [
    "## 7. Conclusion & Future Scope\n",
    "\n",
    "### Conclusion\n",
    "A real-time Indian Sign Language detection system was successfully\n",
    "implemented using computer vision and deep learning.\n",
    "\n",
    "### Future Enhancements\n",
    "- Word and sentence-level recognition\n",
    "- Hindi text output\n",
    "- Mobile application deployment\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
